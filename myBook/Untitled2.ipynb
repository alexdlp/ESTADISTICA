{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469782df-4d16-48d1-a3ba-f6297e2f9ae5",
   "metadata": {},
   "source": [
    "# Markov Models\n",
    "\n",
    "La propiedad markoviana es un ingrediente esencial para el modelo de markov.\n",
    "Para poder entender la propiedad markoviana tenemos que entender lo que normalmente queremos hacer en machine learning cuando tenemos una secuencia.\n",
    "\n",
    "Un secuencia es una lista ordenada de valores o símbolos, como una serie temporal o incluso una frase, la cual es una secuencia de palabras.\n",
    "\n",
    "$$Secuencia: \\{x_1,x_2,...,x_T\\}$$\n",
    "\n",
    "Algo normal e intuitivo es el preguntarse, ¿Cuál es la probabilidad de la secuencia? Es decir, dada una secuencia $ \\{x_1,x_2,...,x_T\\}$, ¿cuál es la probabilidad de **ver** esa secuencia?\n",
    "$$\\text{Probabilidad de la secuencia}: P(x_1,x_2,...,x_T)$$\n",
    "\n",
    "Haciendo esto podemos hacer todo tipo de cosas útiles. Por ejemplo, si sólo conocemos los valores desde $x_1$ hasta $x_{T-1}$, podemos hacernos la pregunta,¿cuál es la distribución de $x_T$ dados los valores previos?\n",
    "\n",
    "$$\\text{Forecast/Predict}: P(x_T|x_{T-1},x_{T-2},...,x_1)$$\n",
    "\n",
    "Esto es útil si lo que se quiere es predecir el próximo valor en una serie temporal.\n",
    "\n",
    "### La propiedad Markoviana\n",
    "\n",
    "Así pues, la propiedad markoviana es un supuesto muy estricto sobre la estructura de dependencia de la distribución de probabilidad conjunta.\n",
    "\n",
    "$$x_{t-2} \\rightarrow x_{t-1} \\rightarrow x_{t} \\rightarrow x_{t+1} \\rightarrow x_{t+2} $$\n",
    "\n",
    "Nos dice que cada $x_t$ depende únicamente valor inmediatamente anterior, $x_{t-1}$. No depende de $x_{t-2}$, $x_{t-3}$ o de ningún otro valor pasado o futuro.\n",
    "\n",
    "¿Qué significa esto en términos de la distribución de probabilidad conjunta? Significa que la probabilidad de la distribución de la serie $P(x_1,...,x_T)$ es igual a $P(x_1) \\cdot P(x_2tx_1) \\cdot P(x_3|x_2) \\cdot ... \\cdot P(x_t|x_{t-1})$\n",
    "\n",
    "$$P(x_1,...,x_T) = P(x_1)\\prod_{t=2}^T P(x_t|x_{t-1})$$\n",
    "\n",
    "El primer $x_1$ no está condicionado a nada, ya que estamos asumiendo que nada viene antes de $x_1$.\n",
    "\n",
    "Otra manera de escribir la propiedad markoviana es decir que la probabilidad de $x_t$ dado todo el pasado, es la misma que la probabilidad de $x_t$ dado el valor inmediatamente anterior.\n",
    "\n",
    "$$P(x_t|x_{t-1},x_{t-2},...) = P(x_t|x_{t-1})$$\n",
    "\n",
    "Otra pregunta interesante que nos podemos hacer es, ¿qué pasaría si la propiedad de markov no fuese cierta?\n",
    "\n",
    "En este caso todavía podemos factorizar la distribución conjunta, solo que no se ve tan bien como en el caso markoviano.\n",
    "\n",
    "para ver cómo podemos hacer esto es útil comenzar con un ejemplo sencillo con solo dos variables: $x_1$ y $x_2$. En ese caso podemos decir:\n",
    "\n",
    "$$P(x_1,x_2) = P(x_1) \\cdot P(x_2|x_1)$$\n",
    "\n",
    "Esto no es más que la regla de Bayes de probabilidad condicionada.\n",
    "\n",
    "¿Si hubiese 3 variables? Podríamos hacer lo mismo:\n",
    "\n",
    "$$P(x_1,x_2, x_3) = P(x_1) \\cdot P(x_2,x_3|x_1) =  P(x_1) \\cdot P(x_2|x_1) \\cdot P(x_3|x_2, x_1) $$\n",
    "\n",
    "En general, a esto se le conoce como la regla de la cadena de la probabilidad (the chain rule of probability).\n",
    "\n",
    "\n",
    "Esencialmente lo que hacemos es dividir repetidamente la distribución, empezando con $x_1$, luego $x_2$, depués $x_3$ y así sucesivamente. Y cada vez que dividimos la distribución, el siguiente valor depende de cada vez más valores pasados.\n",
    "\n",
    "$$P(x_1,...,X_T)=P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)...P(x_T | x_{T-1},...,x_1)$$\n",
    "\n",
    "<div>  <center> <img src=\"markov1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/></center></div> \n",
    "\n",
    "Esta es la forma más general. Pero si asumimos que la propiedad markoviana es cierta, entonces esto se simplifica ya que cada término $x$ no depende de ningún otro excepto del término inmediatamente anteior.\n",
    "\n",
    "Ahora que sabemos que es la propiedad markoviana, veamos por qué es útil.\n",
    "\n",
    "Imaginemos que queremos construir un modelo de la lengua inglesa. Supongamos que en nuestro modelo consideramos las palabras inglesas más utilizadas,alrededor de 2000.\n",
    "\n",
    "Ahora, supongamos que nos gustaría construir una distribución de probabilidad para la décima palabra en una oración,dadas las nueve palabras anteriores, $P(x_{10}|x_9,x_8,...,x_1)$.\n",
    "\n",
    "Entonces esta distribución tiene 10 variables en total $x_1, x_2,...,x_{10}$.\n",
    "\n",
    "Pero cada una de estas diez variables tiene dos mil valores posibles. Entonces, ¿cuál es la dimensionalidad total de esta distribución? $2000 \\cdot 2000 \\cdot ... = 2000^{10}$, un número bastante grande.\n",
    "\n",
    "¿Cuál es el problema de que sea un número tan grande?\n",
    "\n",
    "Bueno, este es el número de probabilidades que tenemos que estimar para poder estimar todos estos números. Debemos tener una cantidad suficiente de datos de los que aprender.\n",
    "\n",
    "A menudo las llamamos propiedad de Markov, la suposición markoviana. Esto se debe a que a menudo suponemos que la propiedad Markov se mantiene, incluso cuando sabemos que no es así.\n",
    "\n",
    "Por ejemplo, con el lenguaje, es bastante obvio que la siguiente palabra en una oración no sólo depende de la palabra precedente.\n",
    "\n",
    "Con este sencillo ejemplo, queda claro que la propiedad de Markov no es válida para el lenguaje, aunque este es un caso de uso muy popular de las cadenas de Markov. \n",
    "\n",
    "Hay una cita popular en estadística que ayuda a arrojar algo de luz sobre esta cuestión.El famoso estadístico George Box dijo una vez que todos los modelos son erróneos, pero algunos son útiles. Es decir, sabemos que la hipótesis de Markov es errónea. Pero a pesar de ello, sigue resultando útil.\n",
    "\n",
    "Ha sido aplicado con éxito en las finanzas, el lenguaje, el aprendizaje por refuerzo y el análisis de secuencias biológicas. Así que, aunque el supuesto de Markov parezca muy restrictivo, sigue siendo útil en el mundo real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e78b1-ad5d-41f2-b429-8e1d2a28f3ef",
   "metadata": {},
   "source": [
    "# El modelo de Markov\n",
    "\n",
    "Vamos a discutir cómo un modelo de Markov se representa matemáticamente y por extensión, en código.\n",
    "\n",
    "El modelo de Markov se utiliza para modelar secuencias.La pregunta es: ¿secuencias de qué?\n",
    "\n",
    "Para el propósito de esta sección, vamos a considerar secuencias de símbolos categóricos, que\n",
    "simplemente llamaremos **estados**.\n",
    "\n",
    "Así, por ejemplo, si estamos modelando el tiempo, podríamos tener tres estados: soleado, lluvioso y nublado.\n",
    "\n",
    "Si estamos modelando el lenguaje, nuestros estados podrían ser etiquetas de partes de la oración, por ejemplo, sustantivo, verbo, adjetivo, etc.\n",
    "\n",
    "Bien, estos son ejemplos de estados que son símbolos categóricos y mediante la creación de un modelo de Markov para estos símbolos, tendremos un modelo para las secuencias de estos estados.\n",
    "\n",
    "<div>  <center> <img src=\"markov2.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/></center></div> \n",
    "\n",
    "En esta sección, usaremos la letra $S$ para significar estado. En otros recursos se utilizan otras letras como $Z$ o $X$, dependiendo del contexto. Pero en esta sección utilizaremos la letra $S$.\n",
    "\n",
    "Ahora, dada esta letra $S$, podemos usar la notación $S(t)$ para significar el estado $S$ en el instante $t$\n",
    "\n",
    "$$S(t) = S_t = \\text{estado en instante t}$$\n",
    "\n",
    "El tiempo también será discreto, siendo los estados número enteros desde 1 hasta $M$, donde\n",
    "$M$ es el número total de estados posibles.\n",
    "\n",
    "En general, cuando queramos referirnos a un estado concreto, utilizaremos la letra I o J.\n",
    "\n",
    "Ahora podemos tener un valor de probabilidad para cada uno de los estados desde 1 hasta $M$. \n",
    "Esto nos da $M$ probabilidades diferentes, y juntas forman una distribución de probabilidad.\n",
    "A esta distribución se la conoce con el nombre **distribución del estado**.\n",
    "\n",
    "Así, por ejemplo, la distribución de estado respondería a la pregunta ¿Cuál es la probabilidad de que llueva el domingo, $P(S_{domingo} = lluvioso)$?\n",
    "\n",
    "\n",
    "\n",
    "Los modelos de Markov consisten en modelizar secuencias en las que cada estado depende únicamente del estado anterior del que venimos. Así que podemos escribir:\n",
    "\n",
    "$$P(S_t = j | S_{t-1}=i)$$\n",
    "\n",
    "Nótese que se trata de una distribución condicional. Ahora, una pregunta que podemos considerar es la siguiente ¿Cuántos de estos valores de probabilidad condicional existen? Bien, debemos tener un valor para cada  $i$ y $j$ posibles desde 1 hasta $M$. Por lo tanto, debemos tener $M^2$ valores.\n",
    "\n",
    "Ahora te habrás dado cuenta de que es bastante tedioso escribir $P(s_t=j|s_{t-1}), \\forall i = 1,...,M$ y $j=1,...,M$\n",
    "\n",
    "Normalmente evitamos escribir esto. En su lugar, podemos simplemente almacenar todos estos valores en una matriz $A$ de dimensiones $M x M$. Llamamos a esto la **matriz de transición de estados**.\n",
    "\n",
    "La matriz de transición de estados para el diagrama de transición de estados anterior es:\n",
    " \n",
    "$$  A =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "    0.8 & 0.15 & 0.05 \\\\\n",
    "    0.2 & 0.5 & 0.3 \\\\\n",
    "    0.2 & 0.2 & 0.6 \\\\\n",
    "  \\end{array} } \\right]$$\n",
    "  \n",
    "La convención es que $A_{ij}$ es igual a la probabilidad de que el estado anterior era $i$ y el estado siguiente es $j$. Por lo tanto, el primer índice corresponde al estado anterior, y el segundo índice corresponde al siguiente estado.\n",
    "\n",
    "\n",
    "Todavía falta una variable. La variable que falta es el tiempo. Entonces, ¿qué pasa con t?\n",
    "En general, estas distribuciones condicionales **pueden cambiar en el tiempo**.\n",
    "\n",
    "Es decir, serán funciones de $t$, pero para el modelo de Markov, este no suele ser el caso. En su lugar, supondremos que las mismas probabilidades se mantienen para todos los valores de $t$, es decir, a lo largo de todo el tiempo. Cuando esto es cierto, decimos que el proceso de Markov es **homogéneo en el tiempo** (para nosotros, siempre será así).\n",
    "\n",
    "Una forma de representar un modelo de markov es con un diagrama de transición de estados.\n",
    "\n",
    "Si todo lo que tenemos es la matriz de transición de estados, ¿tenemos todo lo necesario? La respuesta es no.La matriz $A$ sólo nos dice la probabilidad de pasar de un estado al siguiente, pero cada secuencia comienza en algún lugar. Cada valor nos dice la probabilidad de pasar del anterior estado al siguiente estado. Pero si estamos mirando el primer estado de una secuencia, no hay estado anterior.\n",
    "\n",
    "Para resolver este problema, tenemos que definir otra distribución llamada distribución de estado inicial. Normalmente, utilizamos la letra griega $\\pi$ para representar esta distribución. Así, $\\pi_i = P(S_1 = i)$.\n",
    "\n",
    "Hasta ahora, hemos aprendido que un modelo de Markov es descrito por dos distribuciones, la matriz de transición de estado $A$ y la distribución de estado inicial $\\pi:\n",
    "\n",
    "$$A_{ij} = P(S_t = j |S_{t-1}=i)$$\n",
    "$$\\pi_i = P(S_i = i)$$\n",
    "\n",
    "\n",
    "Dados estos dos objetos, hay dos preguntas que vamos a tratar de responder. La primera pregunta es dado $A$ y $\\pi$l y la secuencia $\\{s_1,s_2,...s_T\\}$ ¿Cuál es la probabilidad de esta secuencia?\n",
    "\n",
    "Y la segunda pregunta que vamos a responder es ¿Cómo encontramos $A$ y $\\pi$ en primer lugar?\n",
    "\n",
    "Como esto es aprendizaje automático, se nos dará un conjunto de datos y usando este conjunto de datos determinaremos algún procedimiento para estimar los mejores valores de  $A$ y $\\pi$.\n",
    "\n",
    "\n",
    "Así que vamos a empezar por abordar la primera pregunta. Dada una secuencia de estados en el modelo de Markov, ¿cómo encontramos la probabilidad de esa secuencia?\n",
    "\n",
    "En general, podríamos aplicar la regla de la cadena de probabilidad, pero gracias a la propiedad de Markov, esto se simplifica al producto de las transiciones de estado.\n",
    "\n",
    "$$P(S_1,...,S_T) = P(S_1)\\prod_{t=2}^T P(S_1 | S_{t-1})$$\n",
    "La diferencia ahora es que utilizaremos nuestras nuevas variables en PI para representar nuestra respuesta.\n",
    "$$P(S_1,...,S_T) = \\pi_{S_1}\\prod_{t=2}^T A_{S_{t-1},S_t}$$\n",
    "\n",
    "\n",
    "Así que la siguiente pregunta que queremos responder es ¿Cómo entrenamos un modelo de Markov? Requeriría algo de trabajo hacer esto completamente de forma rigurosa, así que por ahora, sólo discutiremos la intuición, que es muy fácil de entender. De hecho, no requiere nada más que contar.\n",
    "\n",
    "Así que para empezar, supongamos que lanzamos una moneda un montón de veces, que es una secuencia de cara y cruz. Y nos gustaría saber cuál es nuestra mejor estimación de la probabilidad de que salga cara. Es el número de caras dividido por el número total de lanzamientos de la moneda.\n",
    "$$P(heads) \\approx \\frac{count(H)}{\\text{total tosses}}$$\n",
    "\n",
    "En este caso, se trata de una distribución binaria, también conocida como Bernoulli, pero en nuestro caso, podemos tener $M$ mayor que dos.\n",
    "\n",
    "Por ejemplo, cada estado es una palabra del diccionario inglés. En este caso, la respuesta es similar. Supongamos que hemos cogido toda la Wikipedia y nos gustaría saber cuál es nuestra mejor estimación de la probabilidad de ver la palabra gato. La respuesta es el número de veces que aparece la palabra gato dividido por el número total de palabras del texto.\n",
    "\n",
    "$$P(\"cat\") \\approx \\frac{count(\"cat\")}{\\text{total word count}}$$\n",
    "\n",
    "Entonces, ¿cómo podemos aplicar este principio para estimar  $A$ y $\\pi$ en un modelo de markov? Empecemos por $\\pi$, ya que es un poco más sencillo.\n",
    "\n",
    "$\\pi_i$ es igual al número de veces que cada secuencia comenzó con el estado $i$ dividido por el número total de secuencias de nuestro conjunto de datos.\n",
    "\n",
    "$$\\hat{\\pi}_i = \\frac{counts(S_1=i}{N}$$\n",
    "\n",
    "Por supuesto, esto requiere que nuestro conjunto de datos conste de múltiples secuencias para empezar. De lo contrario, nuestra estimación no sería muy buena.\n",
    "\n",
    "Nuestra mejor estimación de $A_{ij}$ es el número de veces que pasamos del estado $i$ al estado $j$ dividido por el número total de veces que estuvimos en el estado $i$.\n",
    "\n",
    "$$\\hat{A}_{ij} = \\frac{count(i \\rightarrow j}{count(i)}$$\n",
    "\n",
    "Por si esto resulta un poco confuso, pensemos en un ejemplo. Supongamos que trabajamos con la lengua inglesa y que cada estado es una palabra. Nos gustaría saber cuál es la probabilidad de ver la palabra *cat* después de la palabra *the*.\n",
    "\n",
    "Esto es simplemente el número de veces que vemos la secuencia *the cat* dividido por el total de veces que vemos la palabra *the* sola.\n",
    "\n",
    "\n",
    "\n",
    "En esta clase, aprendimos que usando términos más genéricos, nuestro trabajo es modelar una secuencia de estados.\n",
    "\n",
    "Nuestro modelo de Markov se compone de dos objetos la matriz de transición de estados A y la distribución inicial de estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189792e-40ae-4d87-9a50-616f6045505b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Suavizado de probabilidades y espacio logarítmico\n",
    "\n",
    "Vamos a considerar una ligera modificación del método de estimación de $A$ y $\\pi$ y también cómo calcular la probabilidad de una secuencia utilizando el modelo de Markov.\n",
    "\n",
    "En otras palabras, vamos a considerar formas ligeramente diferentes de responder a las dos preguntas que previamente planteamos.\n",
    "\n",
    "Las estimaciones anteriores que discutimos se llaman **estimaciones de máxima verosimilitud**, ya que son las estimaciones que producen el valor máximo de la verosimilitud dado el conjunto de datos sobre el que se entrena.\n",
    "\n",
    "Ahora veremos un problema potencial de utilizar este método y cómo solucionarlo.\n",
    "\n",
    "Vamos a considerar cómo hallamos la probabilidad de una secuencia utilizando $A$ y $\\pi$.\n",
    "\n",
    "$$P(S_1,...S_T)=\\pi_{S_1}\\prod^T_{t=2}A_{S_{t-1},S_T}$$\n",
    "\n",
    "A partir de esta expresión, podemos ver que sólo implica una operación: la multiplicación.\n",
    "Supongamos ahora que uno de estos valores es cero, lo que puede ocurrir si intentamos hallar la probabilidad de una frase que contiene pares de palabras que nunca aparecieron en nuestro conjunto de datos de entrenamiento.\n",
    "\n",
    "Esto es perfectamente posible, ya que nuestro conjunto de datos de entrenamiento será diferente de nuestro conjunto de datos de prueba.\n",
    "\n",
    "Bien, como esta expresión contiene sólo multiplicación y sabemos que cualquier cosa multiplicada por cero es cero, esto hará que toda la expresión cero. \n",
    "\n",
    "Aunque esto parezca que podría ser la respuesta correcta, no lo es, porque probablemente no queramos concluir que una frase es imposible simplemente porque contiene un par de palabras que no aparecen en nuestro conjunto de entrenamiento.\n",
    "\n",
    "Entonces, ¿qué debemos hacer ante este problema?\n",
    "\n",
    "Intuitivamente, lo que nos gustaría hacer es dar una pequeña probabilidad a cada transición posible, aunque nunca se produzcan en el conjunto de entrenamiento. De este modo, evitaremos encontrar ceros durante el testing.\n",
    "\n",
    "La forma más sencilla de hacerlo es añadiendo un suavizado. Simplemente añadimos un 1 falso a cada transición posible.La estimación resultante para $\\hat{A}_{ij}$ tiene entonces este aspecto.\n",
    "\n",
    "$$\\hat{A}_{ij} = \\frac{count(i \\rightarrow j) +1}{count(i) + M}$$\n",
    "\n",
    "En el numerador añadimos un 1,  en el denominador añadimos $M$.\n",
    "\n",
    "Para que una distribución de probabilidad sea válida, todas las probabilidades deben sumar uno. Como le damos un valor de 1 a cada uno de los valores imposibles, también debemos sumar $M$ al denominador. Esto da como resultado que cada fila de $\\hat{A}_{ij}$ sume uno.\n",
    "\n",
    "También es posible utilizar el suavizado de sumar uno para $\\pi$:\n",
    "\n",
    "$$\\hat{\\pi}_i = \\frac{counts(s_1 = i)+1}{N+M}$$\n",
    "\n",
    "De nuevo, sumamos uno arriba y  $M$ abajo.\n",
    "\n",
    "Una extensión sencilla del suavizado por adición es el suavizado por adición de $\\epsilon$. Quizás creamos que añadir 1 es un poco demasiado sesgado.\n",
    "\n",
    "En este caso, lo que hacemos en su lugar es añadir  $\\epsilon$ al numerador y $\\epsilon M$ al denominador. De nuevo, podemos concluir que esto da como resultado que cada fila de $\\hat{\\pi}_i$ sume 1.\n",
    "\n",
    "Así que hasta ahora, hemos considerado cómo podríamos modificar o responder a una de las preguntas que planteamos anteriormente, cómo encontrar un $\\pi$ dado un conjunto de datos de entrenamiento.\n",
    "\n",
    "La siguiente cuestión que vamos a considerar es cómo podemos modificar nuestra respuesta a la otrapregunta, que era: dada una secuencia, ¿cómo encontrar la probabilidad de esa secuencia?Así que vamos a empezar por discutir nuestra motivación de por qué querríamos hacer esto.\n",
    "\n",
    "Comenzaremos por reconocer el hecho de que nuestra probabilidad conjunta implica sólo multiplicar un montón de números juntos. En concreto, estos números son probabilidades.\n",
    "\n",
    "En el caso de la lengua, estas probabilidades serán muy pequeñas, simplemente porque hay muchas palabras en inglés. En muchas aplicaciones, no es raro considerar un vocabulario de 20.000 o 50.000 palabras. Así que estas probabilidades serán probablemente muy pequeñas.\n",
    "\n",
    "Ahora bien, ¿qué ocurre cuando multiplicamos muchos números pequeños? La respuesta es que se hacen aún más pequeños. Por tanto, a medida que multiplicamos más y más números pequeños, nos acercamos más y más a cero.\n",
    "\n",
    "Debido a que los ordenadores no tienen una precisión infinita, eventualmente, llegaremos tan cerca de cero que el ordenador simplemente redondear la respuesta hacia cero. Esto es muy problemático ya que si queremos comparar dos frases que son ambas muy improbables, si ambas terminan dándonos cero, entonces nuestra respuesta no será válida.\n",
    "\n",
    "Ahora bien, esto puede sonar como un problema muy extraño de considerar, pero en la práctica se esto sucede con bastante facilidad. Entonces, ¿cuál es la solución a este problema?\n",
    "\n",
    "La solución es trabajar en el **espacio logarítmico**.\n",
    "\n",
    "Es decir, en lugar de encontrar probabilidades directamente, podemos encontrar probabilidades logaritmicas. Nótese que está bien hacer esta transformación porque lo que a menudo queremos hacer es comparar dos probabilidades.\n",
    "\n",
    "Así, por ejemplo, si tenemos dos frases, queremos saber qué frase es más probable. O si tenemos dos modelos de Markov, podríamos querer saber bajo qué modelo de Markov la secuencia arroja mayor probabilidad.\n",
    "\n",
    "Obsérvese también que esta transformación no cambiaría la respuesta, ya que el logaritmo es una función monotónicamente creciente.\n",
    "\n",
    "Ahora, ¿cómo se resuelve exactamente el problema tomando el logaritmos? Bien, supongamos que tenemos un valor muy pequeño, $10^{-10}$ y tomamos el logaritmo de base diez de este valor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d4e0783-44cb-42dc-8759-98889be1c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor:1e-10\tLogarítmo del valor:-10.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "number = 10**-10\n",
    "print(f\"Valor:{number}\\tLogarítmo del valor:{math.log(number,10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268584e-b976-435f-a54c-157f6daa8a18",
   "metadata": {},
   "source": [
    "Esto simplemente nos da menos diez, valor un ordenador no tiene ningún problema en representar.\n",
    "\n",
    "Consideremos el número diez a la -100.Este número es un factor de diez más pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d552f38-9029-460a-8322-b0f4ef2880da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor:1e-100\tLogarítmo del valor:-100.0\n"
     ]
    }
   ],
   "source": [
    "number = 10**-100\n",
    "print(f\"Valor:{number}\\tLogarítmo del valor:{math.log(number,10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f09ffe6-3b52-48fe-b307-3c35353f06fd",
   "metadata": {},
   "source": [
    "Pero si tomamos el logaritmo de éste, obtenemos -100, que de nuevo es fácil de representar con un ordenador. En estas circunstancias, la probabilidad logarítmica es muy fácil de calcular.\n",
    "\n",
    "Existe una regla para los logaritmos, que establece que el logaritmo de un producto es la suma de logaritmos.De hecho, si sabes que no vas a necesitar los valores de $A$ y $\\pi$ directamente, puedes simplemente almacenar sus valores logarítmicos.\n",
    "\n",
    "\n",
    "$$logP(S_1,...S_T)=log \\pi_{S_1}\\sum^T_{t=2}logA_{S_{t-1},S_T}$$\n",
    "\n",
    "Hay que tener en cuenta que no queremos calcular el producto de probabilidades y después tomar el logaritmo ya que se terminaría tomando el logaritmo de cero, lo que no resuelve el problema. La solución es tomar la suma de las log-probabilidades.\n",
    "\n",
    "Otra cosa que hay que tener en cuenta es que computacionalmente, hacer sumas es más eficiente que hacer multiplicaciones. Por lo que incluso sin tener problemas de *under flow*, es preferible usar las log-probabilidades por temas de eficiencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10b1668-d35f-4b04-bac6-aeb3cf169d1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Las Matemáticas del modelo de Markov\n",
    "\n",
    "Supongamos que trabajamos con un espacio de estados formado por los estados soleado, lluvioso y nublado y nos gustaría saber cuál es la probabilidad de que haga sol dentro de cinco días.\n",
    "\n",
    "\n",
    "Empezaremos con una pregunta parecida pero más básica. ¿Cuál es la probabilidad de que haga sol dentro de un día? Ya conocemos la respuesta a esta pregunta.\n",
    "\n",
    "Bien, sabemos el tiempo que hace hoy porque podemos mirar por la ventana para medirlo nosotros mismos. Supongamos que el tiempo de hoy es soleado, entonces la probabilidad de que mañana sea soleado es simplemente $P(S_t = sunny|S_{t-1}=sunny)$ o $A(sunny|sunny)$\n",
    "\n",
    "Ahora, el punto importante a tener en cuenta sobre esto es que nuestra predicción sobre el tiempo de mañana es en forma de una distribución de probabilidad.\n",
    "\n",
    "Tenemos alguna probabilidad para soleado. Tenemos alguna probabilidad para lluvioso. Y tenemos alguna probabilidad de nublado.\n",
    "\n",
    "Esto hace que sea un poco más difícil ir al siguiente paso, que es hacer la pregunta, ¿Cuál es la probabilidad de que haga sol dentro de dos días?\n",
    "\n",
    "Empecemos suponiendo que esta distribución es una distribución genérica a la que llamaremos $P(S_t) = \\text{distribución del tiempo mañana}$\n",
    "\n",
    "En este caso, el tiempo $t$ representa un día a partir de ahora. Lo que nos gustaría saber a continuación es la probabilidad de que haga sol dentro de dos días,  en el tiempo $t+1$.\n",
    "\n",
    "Puede parecer una pregunta difícil de responder, pero en realidad se trata simplemente de la regla de Bayes o regla de la probabilidad condicional: Si tenemos $P(A|B)$ y $P(B)$, y queremos encontrar $P(A)$, primero podemos encontrar $P(A,B)$ multiplicando ambas distribuciones.\n",
    "\n",
    "$$ P(A,B) = P(A|B) \\cdot P(B) $$\n",
    "\n",
    "El siguiente paso es sumar todos los posibles valores de $B$, lo que llamamos marginalización para obtener $P(A)$.\n",
    "\n",
    "\n",
    "$$ P(A) = \\sum_b P(A,B) = \\sum_b P(A|B=b)\\cdot P(B=b) $$\n",
    "\n",
    "En términos de nuestro problema, podemos escribir esto usando lluvioso, soleado y nublado.\n",
    "\n",
    "$$P(S_{t+1} = sunny) =  P(S_{t+1}=sunny |S_t = sunny)P(S_t = sunny)+P(S_{t+1}=sunny |S_t = rainy)P(S_t = rainy)+P(S_{t+1}=sunny |S_t = cloudy)P(S_t = cloudy)$$\n",
    "\n",
    "Por lo tanto, sabemos encontrar la distribución de estados en $t+1$ dada la distribución de estados 4n $t$. De hecho, estas expresiones son recursivas. Así que al igual que podemos encontrar la distribución de estados en $t+1$, dada la distribución de estados en $t$, también podemos encontrar la distribución de estado en  $t+2, t+3, t+4,...$ usando exactamente la misma fórmula.\n",
    "\n",
    "Ahora ya sabemos cómo responder a la pregunta, ¿Cuál es la probabilidad de que haga sol dentro de cinco días?\n",
    "\n",
    "Ahora, a lo que eventualmente nos gustaría volver es a utilizar los símbolos $A$ y $\\pi$. Ignoramos $\\pi$ de momento y suponemos que lA distribución de estados actual es $P(S_t)$.\n",
    "\n",
    "$$P(S_{t+1}=j) = \\sum^M_{i=1}A_{ij}P(S_t=i)$$\n",
    "\n",
    "El sumatorio anterior se puede expresar en términos de $A_{ij}$.Simplemente tenemos que multiplicar por $A_{ij}$ todos los valores posibles de los estados iniciales.\n",
    "\n",
    "*Como ejercicio, intenta sustituir AIG por la expresión probabilística para convencerte de que\n",
    "esto funciona.*\n",
    "\n",
    "La siguiente cosa a notar es que esto es realmente sólo una multiplicación vector-matriz.\n",
    "\n",
    "\n",
    "Es un vector de tamaño $M$ con una entrada para cada estado posible. Ahora, cuando estamos trabajando con modelos de Markov, es más conveniente para representar el vector de estado como un vector fila en lugar de un vector columna. Si tratamos $P(S_t)$ como un vector fila, es decir, es una matriz de $1xM$, podemos escribir la suma anterior de forma más compacta.\n",
    "\n",
    "$$P(S_{t+1}) = P(S_t)A$$\n",
    "\n",
    "Dada la expresión anterior, sabemos que podemos realizar la misma operación en cada paso de tiempo, para llegar a la siguiente distribución de estados.\n",
    "$$P(S_{t+1} = P(S_t)A$$\n",
    "$$P(S_{t+2} = P(S_{t+1})A = P(S_t) A^2$$\n",
    "$$...$$\n",
    "$$P(S_{t+k}) = P(S_t)A^k$$\n",
    "\n",
    "Así pues, una pregunta natural que cabe hacerse en este punto es, ¿qué ocurre cuando el tiempo se aproxima al infinito? En unas\n",
    "circunstancias ideales descubrimos que la distribución de estados **converge**.\n",
    "\n",
    "$$P(S_{\\infty}=j) = \\lim_{t \\rightarrow \\infty}P(S_t = j | s_1 = i)$$\n",
    "\n",
    "Es decir, no salta de una distribución a otra, sino que se estabiliza en alguna distribución fija. A esto lo llamamos distribución límite.\n",
    "\n",
    "Un hecho importante a tener en cuenta sobre la definición de la distribución límite es lo siguiente: en la expresión de la derecha, el tiempo $t$ va a infinito. Así que nuestro trabajo es encontrar la probabilidad de que el estado será $j$ después de una cantidad infinita de tiempo.\n",
    "\n",
    "Sin embargo, el estado inicial en $t=1$ es $i$, pero $i$ no aparece en el lado izquierdo. En otras palabras, lo que esto significa es que **la distribución límite no depende del punto de inicio**.Si lo hace, entonces no es una distribución límite.\n",
    "\n",
    "Ahora, suponiendo que nuestra cadena de Markov tiene un comportamiento ideal,¿cómo podemos encontrar esta distribución límite?\n",
    "Como infinito más uno sigue siendo infinito, podemos simplemente enchufar infinito en nuestra fórmula de transiciónde estados.\n",
    "\n",
    "v \\rightarrow P(S_{\\infty}) = P(\\infty)A $$\n",
    "\n",
    "Este es un problema familiar del álgebra lineal.En álgebra lineal, tenemos el problema de encontrar valores propios y vectores propios.\n",
    "\n",
    "$$Av = \\lambda v$$\n",
    "\n",
    "¿Por qué son tan especiales los vectores propios y los valores propios? Normalmente cuando multiplicas una matriz por un vector, obtienes otro vector que puede apuntar en cualquier dirección arbitraria. Es decir, la matriz rota el vector.\n",
    "\n",
    "Los vectores propios son vectores especiales que **no son rotados por la matriz $A$**.El eigenvector es un vector especial que no puede ser rotado por $A$ sino sólo escalado, con un factor de escala $\\lambda$.\n",
    "\n",
    "La única cosa que es diferente acerca de esta ecuación en comparación con lo que estamos haciendo ahora es que estos vectores propios se expresan en términos de vectores columna.\n",
    "\n",
    "Los vectores propios no son únicos.Podemos multiplicar ambos lados por cualquier constante. En nuestro caso, podemos forzar la unicidad, ya que tenemos una restricción adicional en la distribución, que es que todos los valores deben sumar 1.\n",
    "\n",
    "Así que volviendo a la ecuación para la distribución límite, podemos ver que esto es sólo un problema de valores propios.\n",
    "\n",
    "Específicamente, la distribución límite es el vector propio de $A$ con el correspondiente valor propio de 1. Esto es un poco diferente de la ecuación de valor propio típica porque utiliza vectores fila, por lo que hay que convertir esto en formato de vector columna mediante transposición de ambos lados.\n",
    "\n",
    "$$P(S_{\\infty})^T = A^T P({\\infty})^T$$\n",
    "\n",
    "Otro concepto similar es el concepto de distribución estacionaria. La distribución estacionaria se define como cualquier distribución en la que cuando la transición de un paso al siguiente, la distribución de estado no cambia.\n",
    "\n",
    "Así que supongamos que llamamos a nuestra distribución estacionaria $A^*$. Entonces, para encontrar la distribución estacionaria podemos simplemente utilizar nuestra ecuación para las transiciones de estado.\n",
    "$$P(S^*) = P(S^*)A$$\n",
    "\n",
    "Algo peculiar es que esta es exactamente la misma ecuación que teníamos para la distribución límite.Así que, ¿la distribución estacionaria es siempre igual a la distribución límite? Pues no.\n",
    "\n",
    "Un ejemplo sencillo de esto es considerar la matriz de transición  $A = \\left[ {\\begin{array}{cc}\n",
    "    0 & 1 \\\\\n",
    "    1 & 0 \\\\\n",
    "  \\end{array} } \\right]$. Se trata de un modelo de Markov de dos estados en el que siempre se pasa al otro estado en cada paso. En este caso, no existe una distribución límite porque la distribución de estados sigue invirtiéndose en cada paso. Cuando estamos en el estado uno, siempre pasamos al estado dos, y cuando estamos en el estado dos, siempre vamos al estado uno. Así que no converge.\n",
    "\n",
    "Sin embargo, todavía se puede resolver que para la distribución estacionaria es $P(S^*) = (\\frac{1}{2},\\frac{1}{2})$. Básicamente, la regla es que la distribución límite cuando existe es una distribución estacionaria. Pero lo contrario no es cierto.\n",
    "\n",
    "Es decir la distribución estacionaria no es siempre una distribución límite, como hemos visto.\n",
    "\n",
    "Otro caso extraño con el que uno se puede encontrar es cuando la matriz $A$ es la identidad, $A = \\left[ {\\begin{array}{cc}\n",
    "    1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "  \\end{array} } \\right]$.\n",
    "\n",
    "En este caso, todas las distribuciones de estado son distribuciones estacionarias ya que cualquier vector multiplicado por la identidad sigue siendo el mismo vector. En este caso, diríamos que no existe distribución limitante.\n",
    "\n",
    "La definición precisa de la distribución límite requiere que no independientemente del punto de inicio, siempre debes acabar en la misma distribución límite si esa distribución límite existe.\n",
    "\n",
    "Con este ejemplo, se puede ver que si empiezas en el estado$i$I, siempre acabarás en el estado $i$, y por lo tanto, el comportamiento límite sí depende de dónde empieces.\n",
    "\n",
    "### Teorema de *Perron-Frobenius*\n",
    "\n",
    "Este teorema nos ayuda a conectar la distribución límite y la distribución estacionaria. Esencialmente, dice que si cada elemento en nuestra matriz de transición de estado es positivo, entonces la distribución limitante es igual a la distribución estacionaria, y además, esta distribución es única.\n",
    "\n",
    "Es decir, es la única distribución límite y la única distribución estacionaria.\n",
    "\n",
    "Este teorema es realmente útil en escenarios prácticos. Nos permite saber cuándo hemos encontrado la distribución límite,la cual podemos resolver sólo haciendo eigen decomposition sin tener que evaluar un límite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8063f89-1214-464a-a201-f6ee336f9666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
