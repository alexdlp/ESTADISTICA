{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73aefcdb-94ff-4718-b62b-ce6f896dd323",
   "metadata": {},
   "source": [
    "# Ley de los grandes números y Teorema Central del Límite\n",
    "\n",
    "## Muestras aleatorias simples\n",
    "\n",
    "El pilar básico sobre el que se sustenta toda la estadística inferencial es el concepto de **muestra aleatorias simple** o m.a.s. \n",
    "\n",
    "Al final la estadística y la probabilidad son las dos caras de la misma moneda. Una se encarga de de estudiar los sucesos antes de que ocurran -la probabilidad- mientras que la estadística es la rama que estudia los sucesos a posteriori, después de que hayan ocurrido.\n",
    "\n",
    "después de que ocurran, anoto esos datos y los estudio. Precisamente esos datos son una muestra aletoria simple. Desde el punto de vista de la probabilidad, una m.a.s. es una distribución de $n$ variables aleatorias $X_1,...,X_n$ todas independientes entre sí e idénticamente distribuídas ua que queremos simular la repeticion de un experimento $n$ veces de forma independiente.\n",
    "\n",
    "Por tanto, estudiar una muestra aleatoria simple equivale a estudiar su distribución.\n",
    "\n",
    "En muchos casos nos bastara estudiar la distribución de una variable que \"represente\" a dicha m.a.s. Es más facil estudiar una única variable aleatoria que estudiar $n$ variables.\n",
    "\n",
    "Por ejemplo la media muestral definida como $\\bar{X} = \\frac{X_1+...+X_n}{n}$ es una única variable aleatoria que representa o resume cada una de las muetras o variables de la m.a.s.\n",
    "\n",
    "Precisamente me hace falta alguna forma de garantizar que esa variable aleatoria que resume a todas las anteriores siquiera tiene sentido. Ese resultado será la **ley de los grandes números**, que nos dice que la media muestras y la media poblacional se \"parecen\" a la larga o cuando el número de repeticiones $n$ tiende a infinito.\n",
    "\n",
    "\n",
    "El resultado cumbre de este tema seá el **Teorema Central del límite** que nos va a decir que la distribución de la media muestral tiende, sea cual sea la distribución de variables $X_i$, a una normal. \n",
    "\n",
    "### La distribución de la media muestral\n",
    "\n",
    "Sean $X_1,...,X_n$ $n$ variables normales de media $\\mu$ y varianza $\\sigma^2$, todas normales e independientes. Consideremos la variable $\\bar{X} = \\frac{X_1+...+X_n}{n}$ la media muestral. Entonces, la distribución de la varialbe aleatoria $\\bar{X}$ es normal de la misma media $\\mu$ de las $X_i$ y varianza $\\frac{\\sigma ^2}{n}$.\n",
    "\n",
    "Osease. Si yo tengo un conjunto de variables normales, cada una de ellas de media $\\mu$ y varianza $\\sigma^2$ y contruyo una v.a. que sea  $\\bar{X} = \\frac{X_1+...+X_n}{n}$, la distribución de esta v.a. es normal de la misma media $\\mu$ de las $X_i$ y varianza $\\frac{\\sigma ^2}{n}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe8572-9530-4c84-a17e-67d70503f697",
   "metadata": {},
   "source": [
    "## Convergencia de sucesiones de variables aleatorias\n",
    "\n",
    "Vamos a intentar concretar como la media muestral y la media poblacional de una m.a.s se van pareciendo, asi como la distribución de la media muestral se va \"acelerando\" a la normalidad.\n",
    "\n",
    "Es decir, si yo quiero saber cuál es el peso medio de un individuo en España tengo dos opciones: o bien me paseo por toda España y pregunto a todo el mundo que me da su peso y hago la media (lo que llamaremos media poblacional); o bien tomamos una muestra aleatoria simple, seleccionamos un subconjunto de toda la población (100 personas), mido su peso y resulta que haciendo la media de ese peso voy a tener una distribución que se acercará a la normalidad.\n",
    "O lo que es lo mismo: la media muestral tenderá a la media poblacional.\n",
    "\n",
    "Para ello, es necesario introducir un conjunto de conceptos relacionados con la convergencia de variables aleatorias.\n",
    "\n",
    "### Sucesión de variables aleatorias\n",
    "\n",
    "Consideremos un experimento aleatorio sobre un **espacio muestral $\\Omega$**. Sea $P$ una probabiliddad definida sobre el conjunto de sucesos de $\\Omega$. Entonces, si $X_1, X_2, ..., X_n,...$ son una serie de variables aleatorias definidas sobre $\\Omega, P$ diremos que forman una **sucesión de variables aleatorias** y lo denotaremos por $\\{X_n\\}_{n=1}^{\\infty}$\n",
    "\n",
    "#### Ejemplo: lanzamiento de un dado\n",
    "Consideremos el experimento aleatorio de ir lanzando un dado no trucado y definimos la variable aleatoria $X_n$ como resultado del dado en el lanzamiento $n$-ésimo.\n",
    "\n",
    "En la primera v.a. alomejor sale un 5, en la segunda un 2... y así.\n",
    "\n",
    "Entonces, la sucesión de v.a. $X_1,...,X_n,...$ sería la asociada al lanzamiento del dado.\n",
    "\n",
    "**¡OJO!** No confundamos la sucesión de variables aleatorias $X_1,...,X_n,...$ con la sucesiín de resultados de dichas v.a. $x_1,...,x_n,...$. lo primero correspondería a variables aleatorias con su función de probabilidad, esperanza, varianza, etc.; y lo segundo sería simplemente una sucesión numérica de valores enteros entre 1 y 6.\n",
    "\n",
    "\n",
    "El primero de los tipos de convergencia que vamos a definir es la convergencia casi segura, A donde tiende casi seguramente esa distribución.\n",
    "\n",
    "### Convergencia casi segura.\n",
    "\n",
    "Sea $X_1,...,X_n,...$ una sucesión de variables aleatorias y sea $X$ una v.a. definida sobre el mismo espacio miestral $\\Omega$ y con la misma probabilidad de sucesos. Diremos que la sucesión $\\{X_n\\}_{n=1}^{\\infty}$ converge **casi seguramente** hacia $X$ si\n",
    "\n",
    "$$P(\\{ w \\in \\Omega| lim_{0 \\rightarrow \\infty} X_n(w) = X(w)\\}) = 1$$\n",
    "\n",
    "Con palabras: La probabilidad de que un suceso del espacio muestral cumple que el limite cuando n tiende a infinito de la v.a. $X_n$ en ese elemento del espacio muestral tome el mismo valor que la v.a. $X_n$ considerada sobre ese mismo suceso sobre el espacio muestral, si esa probabilidad para cada uno de los sucesos del espacio muestral considerados tiende a 1.\n",
    "\n",
    "Se denota como $X_n \\xrightarrow{\\text{c.s.}} X$\n",
    "\n",
    "Qué significa:\n",
    "\n",
    "Si el conjunto de elementos $w$ del espacio muestral $\\Omega$ que cumplen que el límite de la sucesión de números reales $(X_n(w))_n$ tiende a $X(w)$ diremos que converge casi seguramente cuando ese conjunto tenga probabilidad 1.\n",
    "\n",
    "Es decir, estamos garantizando con probabilidad 1 que para cada elemento del espacio muestral, el límite de las v.as $X_n$ va a converger a la v.a. $X$\n",
    "\n",
    "Comprobar la convergencia casi segura a partir de la función puede ser muy complicado. Por suerte, la siguiente proposición nos ayuda:\n",
    "\n",
    "Sea $X_1,...,X_n,...$ una sucesión de v.as y sea $X$ una v.a. definida sobre el mismo espacio muestral $\\Omega$ y con la misma probailidad de sucesos. Entonces $X_n \\xrightarrow{\\text{c.s.}} X$ si, y solo si, para todo valor $\\epsilon > 0 $ la siguiente serie\n",
    "\n",
    "$$\\sum^{\\infty}_{n=1}P(|X_n -X| > \\epsilon)$$\n",
    "\n",
    "es convergente.\n",
    "\n",
    "Es decir, cuando sumamos que tan probable es que cada una de las v.a. $X_n$ se aleje de $X$ una cantidad superior a $\\epsilon$, esos números son tan pequeños que la serie converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a09499-f595-4849-82c1-6e81f20d928b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### EJEMPLO DADO\n",
    "\n",
    "Veamos si la sucesión $\\{X_n\\}_{n=1}^{\\infty}$ tiene convergencia **casi segura** hacia la variable $X$.\n",
    "\n",
    "Primero vamos a definir algunas cosas:\n",
    "\n",
    "* Variable aleatoria $X_n$: es la variable que cuenta el número que sale en la tirada $n$-ésima del dado.\n",
    "\n",
    "* Variable aleatoria $X$: es la variable aleatoria que modeliza el comportamiento del dado. Es decir, cada tirada es una v.a. $X$ que puede tomar valores $x$ del dominio $\\Omega$.\n",
    "\n",
    "* La función de probabilidad de un dado (de la v.a. $X$):\n",
    "\n",
    "$$P_X(X=x)= \\left\\{ \\begin{array}{lcc}\n",
    "             \\frac{1}{6} \\hspace{0.5em}si\\hspace{0.5em} x =1,2,3,4,5,6\\\\\n",
    "             \\\\ 0 \\hspace{0.5em}en\\hspace{0.5em}otro\\hspace{0.5em}caso \\\\\n",
    "             \\end{array}\n",
    "   \\right.$$\n",
    "   \n",
    "* El espacio muestral de un dado es (de la v.a. $X$):\n",
    "\n",
    "$$\\Omega = \\{1,2,3,4,5,6\\}$$\n",
    "\n",
    "Ahora vamos a contruir otra variable aleatoria que será la diferencia entre $ X_n$ y $X$: $$D_n = X_n - X$$\n",
    "\n",
    "Siendo su espacio muestral $D_n(\\Omega) = \\{-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5\\}$. \n",
    "\n",
    "Este espacio muestra se construye de la siguiente manera: \n",
    "* Cuando $X_n = 1$ y $X = 6 \\rightarrow D_n = 1-6 = -5 $  \n",
    "* Cuando $X_n = 2$ y $X = 6 \\rightarrow D_n = 2-6 = -4 $ \n",
    "* ...\n",
    "* Cuando $X_n = 5$ y $X = 5 \\rightarrow D_n = 5-5 = 0 $\n",
    "* ...\n",
    "* Cuando $X_n = 6$ y $X = 1 \\rightarrow D_n = 6-1 = 5 $  \n",
    "\n",
    "Para aplicar la proposición anterior vamos a hallar la **función de probabilidad** de la variable $D_n$. \n",
    "\n",
    "Para ello, primero hay que hallar la **función de probabilidad conjunta** de la variable $(X_n,X)$.\n",
    "Al ser $X_n$ y $X$ independientes, su FDP será el producto de las probabilidades marginales de ambas variables:\n",
    "$$P_{D_n} = P_{X_n}(X_n = x_n) \\cdot P_X(X=x) = \\frac{1}{6} \\cdot \\frac{1}{6} = \\frac{1}{36}$$\n",
    "\n",
    "Por tanto, la **función de probabilidad de la variable $D_n$** sera:\n",
    "* $P_{D_n}(-5) = P_{X_nX}(1,6)=\\frac{1}{36}$\n",
    "* $P_{D_n}(-4) = P_{X_nX}(2,6)+P_{X_nX}(1,5)=\\frac{2}{36}$\n",
    "* $P_{D_n}(-3) = P_{X_nX}(3,6)+P_{X_nX}(2,5)+P_{X_nX}(1,4)=\\frac{3}{36}$\n",
    "* $P_{D_n}(-2) = P_{X_nX}(4,6)+P_{X_nX}(3,5)+P_{X_nX}(2,4)+P_{X_nX}(1,3)=\\frac{4}{36}$\n",
    "* $P_{D_n}(-1) = P_{X_nX}(5,6)+P_{X_nX}(4,5)+P_{X_nX}(3,4)+P_{X_nX}(2,3)+P_{X_nX}(1,2)=\\frac{5}{36}$\n",
    "* $P_{D_n}(0) = P_{X_nX}(6,6)+P_{X_nX}(5,5)+P_{X_nX}(4,4)+P_{X_nX}(3,3)+P_{X_nX}(2,2)+P_{X_nX}(1,1)=\\frac{6}{36}$\n",
    "* $P_{D_n}(1) = P_{X_nX}(6,5)+P_{X_nX}(5,4)+P_{X_nX}(4,3)+P_{X_nX}(3,2)+P_{X_nX}(2,1)=\\frac{5}{36}$\n",
    "* $P_{D_n}(2) = P_{X_nX}(6,4)+P_{X_nX}(5,3)+P_{X_nX}(4,2)+P_{X_nX}(3,1)=\\frac{4}{36}$\n",
    "* $P_{D_n}(3) = P_{X_nX}(6,3)+P_{X_nX}(5,2)+P_{X_nX}(4,1)=\\frac{3}{36}$\n",
    "* $P_{D_n}(4) = P_{X_nX}(6,2)+P_{X_nX}(5,1)=\\frac{2}{36}$\n",
    "* $P_{D_n}(5) = P_{X_nX}(6,1)=\\frac{1}{36}$\n",
    "\n",
    "Así obtenemos la función de probabilidad de la variable diferencia calculada a partir de la conjunta entre $X_n$ y $X$.\n",
    "\n",
    "Ahora, si $\\epsilon$ es un valor real cualquiera entre 0 y 1, $0< \\epsilon <1$, entonces, el suceso $\\{ |D_n|> \\epsilon \\}$ será el complementario del suceso  $\\{D_n=0 \\}$ porque de todo el espacio muestral $D_n(\\Omega)$, el único valor **que  no cumple  $\\{|D_n|> \\epsilon\\}$** es el valor $D_n = 0$. Por tanto:\n",
    "\n",
    "$$P(|D_n| > \\epsilon) = 1 - P(D_n = 0) = 1 - \\frac{6}{36} =1- \\frac{1}{6} = \\frac{5}{6} $$\n",
    "\n",
    "¿Y cuál es el problema con $P(|D_n| > \\epsilon) = \\frac{5}{6}$? Pues que, volviendo a la ecuación \n",
    "$\\sum^{\\infty}_{n=1}P(|X_n -X| > \\epsilon)$, para el caso del dado, $\\sum^{\\infty}_{n=1}\\frac{5}{6}$ no es convergente de forma obvia.\n",
    "\n",
    "Por tanto, deducimos que la sucesión $\\{X_n\\}_{n=1}^{\\infty}$ no converge **casi seguramente** hacia la variable $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de8ec55-03c7-4074-aaf9-72651120bc19",
   "metadata": {},
   "source": [
    "## Convergencia en Probabilidad\n",
    "\n",
    "Sea $X_1,...,X_n,...$ una sucesión de v.as y sea $X$ una v.a. definida sobre el mismo espacio muestral $\\Omega$ y con la misma probailidad de sucesos. Diremos que la sucesión $\\{X_n\\}_{n=1}^{\\infty}$ converge **en probabilidad** hacia $X$ si para cualquier valor $\\epsilon > 0$, el límite cuando $n$ tiende a infinito de la probabilidad que el valor absoluto de la diferencia entre $X_n-X$ se eleje de $\\epsilon$ tiende a cero.\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} P(|X_n-X| > \\epsilon)=0$$\n",
    "\n",
    "Se denota como $X_n \\xrightarrow{\\text{c.p.}} X$\n",
    "\n",
    "\n",
    "Fijaros que en este caso es como si hubiera extraído el límite fuera de la propiedad anterior porque ahora lo único que me interesa es que el límite cuando $n$ tiende a infinito de que la probabilidad de que $X_n-X > \\epsilon$, que ese límite tienda a cero. La propiedad anterior requería que la suma de todos esos valores convergiera.\n",
    "\n",
    "Claro, que la serie $\\sum^{\\infty}_{n=1}P(|X_n -X| > \\epsilon)$ converja tiene un requisito muy fuerte. De hecho una condición que es necesaria aunque no suficiente para con la serie converja es que el límite sea cero.\n",
    "\n",
    "Por tanto la convergencia casi segura es claramente una restricción muchísimo más fuerte que ésta que estoy proponiendo aquí.\n",
    "\n",
    "Por eso la convergencia en probabilidad aligera un poquito los resultados.\n",
    "\n",
    "Es decir, en este caso lo que quiero estudiar es si el límite de la probabilidad de que los sucesos formados por el $w \\in \\Omega$ tal que $ |X_n(w)-X(w)| > \\epsilon$, si esa probabilidad puede tender a 0.\n",
    "\n",
    "Una definición equivalente de convergencia en probabilidad y que a veces se utiliza es utilizando el complementario en la definición anterior. Por tanto sería decir que el límite cuando $n$ tiende a infinito de que la probabilidad de que el valor absoluto de la diferencia de $X_n(w)-X(w)$ sea menor o igual que $\\epsilon$ tiene un límite 1.\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} P(|X_n(w)-X(w)| \\leq \\epsilon)=1$$\n",
    "\n",
    "En este caso la **convergencia casi segura** implica claramente la **convergencia en probabilidad** ya que si una sucesión $\\{X_n\\}_{n=1}^{\\infty}$ converge casi seguramente a $X$, la serie $\\sum^{\\infty}_{n=1}P(|X_n-X| > \\epsilon)$ será convergente. Si esa serie es convergente, una condición necesria es que el límite de su término $P(|X_n-X|> \\epsilon)$ tienda a cero, y si esa y si esa probabilidad tiende a cero equivale precisamente a la convergencia en probabilidad.\n",
    "\n",
    "\n",
    "Por tanto como la convergencia casi segura es algo súper estricto y que no todas las variables cumplen -como hemos visto en el ejemplo anterior- lo que hacemos es quitarle un poquito de condiciones para buscar algo que no sea tan estricto, una convergencia que no sea tan cerrada y hablamos en este caso de la convergencia en probabilidad.\n",
    "\n",
    "Por tanto, primer resultado: **la convergencia casi segura implica la convergencia en probabilidad** pero evidentemente no al revés porque existen series que tienen límite cero pero no convergen.\n",
    "\n",
    "Este resultado me va a ayudar algunas veces a comprobar la convergencia en probabilidad.\n",
    "\n",
    "Y es que si $X_1,...,X_n,...$ una sucesión de v.a. y denotamos su esperanza como $E(X_n) = \\mu_n$ y varianza como $Var(X_n) = \\sigma_n^2$; si suponemos que el límite cuando $n$ tiende a infinito de la varianza ($\\lim_{n \\rightarrow \\infty} \\sigma_n^2 = 0$), entonces, la variable $X_n$ menos $\\mu_n$ tinde en probabilidad a 0.\n",
    "\n",
    "$$X_n - \\mu_n \\xrightarrow{\\text{c.p.}} 0 $$\n",
    "\n",
    "## Demostración\n",
    "\n",
    "La demosrtración se puede hacer de forma muy sencilla mediante la desigualdad de Chebyschev. Mediante la desigualdad de Chebychev podemos escribir que la probabilidad de que $X_n-\\mu_n$ se aleje de $\\epsilon$ está acotado por $\\frac{\\sigma_n^2}{\\epsilon^2}$ para cada uno de los valores de la v.a. $X_n$\n",
    "$$P(|X_n-\\mu_n| > \\epsilon) \\leq \\frac{\\sigma_n^2}{\\epsilon^2}$$\n",
    "\n",
    "Si ahora tomo el límite cuando $n \\rightarrow \\infty$ a cada una de estas dos partes:\n",
    "\n",
    "$$0 \\leq lim_{n \\rightarrow \\infty} P(|X_n-\\mu_n| > \\epsilon) \\leq  lim_{n \\rightarrow \\infty} \\frac{\\sigma_n^2}{\\epsilon^2} = 0$$\n",
    "\n",
    "Pero por hipotesis $\\lim_{n \\rightarrow \\infty} \\sigma_n^2 = 0$, por tanto, al dividirlo por una constante, $\\epsilon$, tenderá a cero.\n",
    "\n",
    "Por tanto como la expresión $lim_{n \\rightarrow \\infty} P(|X_n-\\mu_n| > \\epsilon)$ tiene que ser a la vez mayor o igual y menor o igual que 0, dedicimos directamente que $lim_{n \\rightarrow \\infty} P(|X_n-\\mu_n| > \\epsilon) = 0 $ tal como queríamos ver.\n",
    "\n",
    "### Volviendo al dado\n",
    "\n",
    "Si tomamos el ejemplo anterior del dado, no hay convergencia en probabilidad ya que comprobamos que para $0< \\epsilon < 1$, \n",
    "$$P(|X_n-X|> \\epsilon) = \\frac{5}{6}$$\n",
    "\n",
    "Por tanto, para $0< \\epsilon < 1$, $lim_{n \\rightarrow \\infty} P(|X_n-\\mu_n| > \\epsilon) =\\frac{5}{6} \\neq 0 $\n",
    "\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "Consideremos una v.a. $X_n$ exponencial de parámetro $\\lambda n$ con función de densidad continua: \n",
    "\n",
    "$$f_{X_n}(x) = \\lambda n e^{- \\lambda n x}$$\n",
    "\n",
    "Veamos como la sucesión de v.a. $X_n$ desde $n=1$ hasta infinito converge en probabilidad hacia cero.\n",
    "Para ello, dado un $\\epsilon$ positivo , $\\epsilon > 0$, calculamos la probabilidad  \n",
    "de que la diferencia entre $|X_n-0|$ es mayor que $\\epsilon$ (si quiero ver que converge hacia la distribución 0 tendría que hacer menos 0)\n",
    "\n",
    "$$P(|X_n-0|> \\epsilon) = P(|X_n|> \\epsilon)= \\int_{\\epsilon}^{\\infty} \\lambda n e^{- \\lambda nx} dx = \\lambda n[\\frac{1}{- \\lambda n}e^{- \\lambda n x}]_{\\epsilon}^{\\infty} = e^{- \\lambda n x} \\xrightarrow{n \\rightarrow \\infty} 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06e004-e0cb-4f44-a0ea-fee215be2136",
   "metadata": {},
   "source": [
    "## Convergencia en ley o en distribución\n",
    "\n",
    "Después de la convergencia casi segura y la convergencia en probabilidad nos damos cuenta de que en algunos casos todavía tenemos una restricción muy fuerte.\n",
    "\n",
    "Esto es muy típico en matemáticas; cuando definimos algo de forma tan restringida que sólo lo cumplen unos pocos elementos intentamos quitar condiciones y liberar las restricciones para que sean más suaves.\n",
    "\n",
    "La convergencia en probabilidad ya era un poquito menos restrictiva que la casi segura pero todavía podemos ser mucho mucho más ligeros.\n",
    "\n",
    "Y es que la **convergencia en ley o la convergencia en distribución** le va a pasar la pelota de la probabilidad a la distribución de la variable aleatoria.\n",
    "\n",
    "De modo que si tenemos una sucesión $X_1, ..., X_n,...$ de v.a. y $X$ una v.a. definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos, si $F_{X_n}$ y $F_X$ son las funciones de distribución de la variable $X_n$ y $X$ respectivamente, diremos que la sucesión de v.a. $\\{X_n\\}_{n=1}^{\\infty}$ converge **en ley o en distribución** hacia $X$ si el límite cuando $n$ tiende a infinito de la función de distribución de $X_n$ evaluada en $x$ converge o da la función de distribución de la variable aleatoria $X$ para todo $X$.\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} F_{X_n}(x) = F_X(x)$$\n",
    "\n",
    "Se denota por $X_n \\xrightarrow{\\mathcal{L}} X$\n",
    "\n",
    "Tal vez esta sea la más fácil de entender de las 3 porque al final para cada variable aleatoria $X_n$ tendremos una función de distribución y lo que me interesa es que cuando $n$ tiende a infinito la función de distribución de $X_n$ se acerque, o dé la función de distribución de la v.a. $X$. Esta es la menor de las convergencias o una de las que menos restricciones tiene.\n",
    "\n",
    "En particular, todavía ya una forma de simplificar el resultado. Para ello vamos a utilizar las funciones características.\n",
    "\n",
    "Sea $\\{X_n\\}_{n=1}^{\\infty}$ una sucesión de v.a. y sea $X$ una v.a. definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos. Sean $\\phi_{X_n}$ y $\\phi_X$ las funciones características de $X_n$ y $X$ respectivamente. Entonces, la sucesión converge **en ley** hacia $X,X_n \\xrightarrow{\\mathcal{L}} X$ si y solo si\n",
    "\n",
    "$$lim_{n \\rightarrow \\infty} \\phi_{X_n}(t) = \\phi_X(t) $$\n",
    "para cualquier número $t \\in \\mathbb{R}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be636191-f288-449b-9a8a-f1d2335aff34",
   "metadata": {},
   "source": [
    "## Ejemplo Binomial $B(n,p)$\n",
    "\n",
    "Veamos que si $X_n = B(n,p_n)$ con $p_n = \\frac{\\lambda}{n}$ y $\\lambda$ constante,\n",
    "$$B(n,p)  \\xrightarrow{\\mathcal{L}} Poiss(\\lambda) $$\n",
    "\n",
    "En este caso, a medida que hacemos tender $n$ a infinito, sería un experimento binomial que tiene más intentos pero cada vez la probabilidad $p_n$ es más pequeña porque $\\lambda$ es constante.\n",
    "\n",
    "Asique la binomial con parámetro $n$ y peso $p_n$ tiende en ley o en distribución a una Poisson de parámetro $\\lambda$.\n",
    "\n",
    "Para todo $k \\in \\{0,...,n \\}$ la probabilidad de que una $B(n,p)$ valga $k$ y haciéndolo tender a infinito, se acerca a la probabilidad de que una $Poiss(\\lambda)$ tome el valor $k$:\n",
    "\n",
    "$$P(X_n = k)= \\binom{n}{k} \\cdot p_n^k \\cdot (1-p_n)^{n-k} \\xrightarrow{n \\rightarrow \\infty} P(X=k)=\\frac{\\lambda !}{k!} \\cdot e^{- \\lambda}$$\n",
    "\n",
    "¡Ojo! Esto lo único que significa es que la probabilidad de que una $B(n,p)$ valga $k$ es lo mismo, o se parece mucho a la probabilidad de que una $Poiss(\\lambda)$ valga $k$. Esto no significa todavía que sea una convergencia en ley o en distribución o en probabilidad o nada.\n",
    "\n",
    "Sin embargo tenemos que dado $x \\in \\mathbb{R}$ siempre existe un $k \\in \\{0,...,n\\}$ tal que $x$ esté acotado entre $k \\leq x <k+1$. Por tanto, cuando hago el límite $\\lim_{n \\rightarrow \\infty}F_{X_n}(x)$ es lo mismo que evaluarlo en $k$ en vex de en $x$ porque desde $k$ hasta $x$, ese pequeño grupito de decimales no acumula nada de probabilidad puesto que estamos en un caso discreto. Esto significa que sería el límite cuando $n$ tiende a infinito de la suma \n",
    "de la $X_n = 0$ más la probabilidad de que $X_n = 1$ más la probabilidad de que $X_n = 2$, etcétera hasta la probabilidad de que $X_n = k$. Sería la suma de probabilidades desde 0 hasta $k$, que es el concepto de función de distribución acumulada.\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty}F_{X_n}(x) = \\lim_{n \\rightarrow \\infty}F_{X_n}(k) =   \\lim_{n \\rightarrow \\infty}P(X_n =0) + P(X_n =01)+...+P(X_n =k)$$\n",
    "\n",
    "Pero el límite de la suma es la suma de límites, así que podríamos hacer\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty}P(X_n =0) +...+\\lim_{n \\rightarrow \\infty}P(X_n =k)$$\n",
    "\n",
    "Aplicando la propiedad anterior (lo de la binomial que vale lo mismo que la poisson), esto sería exactamente lo mismo que considerar que una $Poiss(\\lambda)$ valga 0, más una que valga 1...\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty}P(X_n =0) +...+\\lim_{n \\rightarrow \\infty}P(X_n =k) = P(X_n =0) +...+P(X_n =k) $$\n",
    "\n",
    "Y sumar desde cero hasta $k$ las probabilidades de un $\\lambda$ fijo es la función de distribución de $X$, de la $Poiss(\\lambda)$ evaluada en $k$ y que, como seguimos es discreto, es lo mismo que la función de distribución de $X$, de la Poisson evaluada en $x$ porque el trocito que va desde $k$ hasta $x$, como solo tiene decimales, no acumularía nada de probabilidad puesto que el dominio de la Poisson sigue siendo discreto, así que \n",
    "\n",
    "$$F_X(k) = F_X(x)$$\n",
    "hemos demostrado que el límite de las funciones de probabilidad de las $B(n,p_n)$ con $p_n =\\frac{\\lambda}{n}$ converge o tiende en ley a una Poisson de ese mismo parámetro $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586172e-ee3d-408c-a69c-0fddcb12ecc8",
   "metadata": {},
   "source": [
    "## Relación entre las 3 convergencias\n",
    "\n",
    "El resultado siguiente nos dice cuando unh tipo de convergencia implica otra.\n",
    "\n",
    "Si una sucesión de v.a. $\\{X_n\\}_{n=1}^{\\infty}$, sinedo $X$ una v.a. definida sobre el mismo espacio muestral $\\Omega$ y con la misma probabilidad de sucesos, entonces:\n",
    "\n",
    "* Si $X_n \\xrightarrow{\\text{casi seguramente}} X$, entonces $X_n \\xrightarrow{\\text{converge en probabilidad}} X$\n",
    "* Si $X_n \\xrightarrow{\\text{converge en probabilidad}} X$, entonces $X_n \\xrightarrow{\\text{converge en ley}} X$\n",
    "\n",
    "En resumen: la convergencia más fuerte es la casi segura, luego vendría la convergencia **en probabilidad** y por último, la convergencia **en ley**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988c1ae-9fb0-40ee-afed-3703f11d94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "FALTAN VARIOS VIDEOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955e9a2-352c-408f-ae3b-e2f499ad50d0",
   "metadata": {},
   "source": [
    "# TEOREMA CENTRAL DEL LÍMITE\n",
    "\n",
    "Llegamos al resultado fundamental al pilar de toda la estadística y al nexo de unión entre variables aleatorias y estadística.\n",
    "\n",
    "Recapitulemos algunas cosas que ya sabemos necesarias para la demostración del teorema.\n",
    "\n",
    "Sabemos que si una sucesión $\\{X_n \\}$ está formada por variables normales, la sucesión de medias muestrales $\\left\\{ \\bar{X}_n  = \\frac{\\sum^n_{i=1}X_i}{n}\\right\\}_{n=1}^{\\infty}$ también son normales ya que si aplicamos una transformación afín a una variable normal multidimensional, el resultado es una normal.\n",
    "\n",
    "Para calcular la variable $\\bar{X}_n$, es obvio que la transformación lineal es la siguiente:\n",
    "\n",
    "$$\\bar{X}_n = \\left( \\frac{1}{n},...,\\frac{1}{n} \\right) \\begin{pmatrix}\n",
    "    X_1\\\\\n",
    "    \\vdots\\\\\n",
    "   X_n\n",
    "  \\end{pmatrix}$$\n",
    "  \n",
    "Si además la sucesión de variables $X_n$ son normales todas con media $\\mu$ y varianza $\\sigma^2$, la sucesión de las medias $\\left\\{ X_n \\right\\}_{n =1}^{\\infty}$ también serán normales de media $\\mu$ y varianza $\\frac{\\sigma^2}{n}$.\n",
    "\n",
    "Estandarizando las variables anteriores, podemos concluir que las variables medias entándarizadas $Z_n = \\left\\{ \\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\right\\}_{n =1}^{\\infty}$ todas son $N(0,1)$.\n",
    "\n",
    "El **Teorema Central de Límite** generaliza el resultado anteiore en el sentido de que si las variables $X_n$ no tienen por qué tener la distribución normal pero son independientes e idénticamente distribuídas, las variables $Z_n$ correspondientes tienden **en ley** a una distribución normal estándar $N(0,1)$\n",
    "\n",
    "En general, se dice que los valores medios de cualquier secuencia de números aproximadamente corresponde a una normal, y ese es el resultado clave y el que permite que la estadística casi todo pase por la variable aleatoria normal estándar.\n",
    "\n",
    "**TEOREMA CENTRAL DEL LÍMITE:** Si una sucesión de v.a. $\\{X_n\\}_{n=1}^{\\infty} $ i.i.d. con $E(X_n) = \\mu$ y $Var(X_n)=\\sigma^2$ para todo $n$, entonces:\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^n X_i - n \\mu}{\\sigma \\sqrt{n}}  \\xrightarrow{\\mathcal{L}} N(0,1) $$\n",
    "\n",
    "En particular el enunciado equivalente de este teorema central del límite es que si dividímos arriba y abajo, en el numerador y denominador por $n$, quedaría simplemente:\n",
    "\n",
    "$$\\frac{\\bar{X}_n - \\mu}{\\frac{\\sigma}{ \\sqrt{n}}}  \\xrightarrow{\\mathcal{L}} N(0,1) $$\n",
    "\n",
    "QUEDA EL RESTO DEL VIDEO min 4.37 o así"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bcf50-32bc-481d-a4bf-397a456fcc19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
